---
layout: post
title: Markov's Next Tweet
date: '2011-04-20T18:54:00.000-07:00'
author: Oatzy
tags:
- graph
- markov chain
- python
- infographic
- twitter
- tweets
- flowchart
- writing
- hand-waving
modified_time: '2011-04-21T07:23:20.415-07:00'
thumbnail: http://4.bp.blogspot.com/-gq8k8xJ_ZNA/Ta9rgibSKEI/AAAAAAAAAgE/1dMSb8vdwuA/s72-c/two-cities.jpg
blogger_id: tag:blogger.com,1999:blog-14769935.post-49258952753402248
blogger_orig_url: https://oatzy.blogspot.com/2011/04/markovs-next-tweet.html
---

<blockquote><i>Have cookie. All is a little intrusive So close to be Everyone is we haven't blogged in mall bathroom -!</i></blockquote>If you've ever taken the time to read spam - and you really should - you might have come across something like this; text that almost seems human, but on closer inspection just doesn't quite make sense.<br /><br /><br /><b>Markov Chains</b><br /><br />I've mentioned <a href="http://en.wikipedia.org/wiki/Markov_chain">Markov Chains</a> before, for example, in the <a href="http://oatzy.blogspot.com/2011/01/snakes-and-ladders.html">Snakes and Ladders</a> post. Here's the formal definition from Wikipedia,<br /><blockquote>A <b>Markov chain</b>, named for <a href="http://en.wikipedia.org/wiki/Andrey_Markov">Andrey Markov</a>,  is a mathematical system that undergoes transitions from one state to  another (from a finite or countable number of possible states) in a  chain-like manner. It is a <a href="http://en.wikipedia.org/wiki/Stochastic_process" title="Stochastic process">random process</a> endowed with the <a href="http://en.wikipedia.org/wiki/Markov_property">Markov property</a>: the next state depends only on the current state and not on the past.</blockquote>In the context of snakes and ladders, this basically says that the square you'll land on in the next turn depends on the square you're currently on, and the roll of a dice. <br /><br /><br /><b>What the Dickens?</b><br /><br />So what does this have to do with text generating?<br /><br />Here's a visual example of how it works. These are the first few lines from <a href="http://en.wikipedia.org/wiki/A_Tale_Of_Two_Cities">A Tale of Two Cities</a> as a flowchart,<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/-gq8k8xJ_ZNA/Ta9rgibSKEI/AAAAAAAAAgE/1dMSb8vdwuA/s1600/two-cities.jpg" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="216" src="http://4.bp.blogspot.com/-gq8k8xJ_ZNA/Ta9rgibSKEI/AAAAAAAAAgE/1dMSb8vdwuA/s400/two-cities.jpg" width="400" /></a></div>In the above, you have the lines broken up word-wise, and linked by which words follow each other. To generate a sentence, pick a starting point and follow the arrows. When you get to a junction pick which way you go at random.<br /><br />So for text generation, your 'current state' will be some word - say, '<i>of</i>' in the above. The <a href="http://en.wikipedia.org/wiki/Markov_chain#Markov_text_generators">Markov text generator</a> then picks a word to follow it. In the above, there are 5 words that could follow '<i>of</i>', each with an equal chance of being chosen; except for '<i>times</i>' which is twice as likely as the others, since it appears twice in the source. <br /><br />It should be noted that it isn't just shuffling the words completely at random  - words are chosen based on what they appear together with in the source.<br /><br /><br /><b>Tweets Go In, Gibberish Comes Out</b><br /><br />By feeding different source texts in to a generator, you can get all variety of different results; to some extent matching the style of the source writer. This is what can make the output seem so human, while being so beautifully nonsensical.<br /><br />In fact, the quote at the very top of the post was generated by "<a href="http://yes.thatcan.be/my/next/tweet/">That Can Be My Next Tweet</a>".<br /><br />The only explanation the site gives as to how it works is,<br /><blockquote><i>This page generates your future tweets based on the DNA of your existing messages.</i></blockquote>But it seems safe to assume, from that and what it outputs, that it's using Markov text generation with your - or in this case my - previous tweets as the source text.<br /><br /><br /><b>What if I don't really is</b><br /><blockquote><i>He opened the door and got into the car engine shuddered into life and the vehicle lurched down the driveway..</i></blockquote>That bit of text is actually an extract from "<a href="http://writebadlywell.blogspot.com/2011/04/forget-what-youre-doing-halfway-through.html">How to write badly well: Forget what you're doing halfway through a sentence</a>", and is human written. But it demonstrates the point well.<br /><br />In the above text, you have two sentence fragments either side of '<i>the car</i>'; each makes sense individually, but not when they're put together.<br /><br />And unfortunately, these sudden changes of direction are a major trip up point for Markov text; especially since this sort of thing can happen multiple times in a single sentence.<br /><br /><br /><b>Letters and Words</b><br /><br />You don't have to chop up the source text word-wise. <br /><br />You could, for example, run a Markov chain <a href="http://ohthehugemanatee.net/word-o-matic/">letter-wise</a> (or even by groups of letters).<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/-Ffs7LRM4v48/TbA9xl4XbzI/AAAAAAAAAgI/OG1U3O6Oluk/s1600/best-worst.jpg" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="320" src="http://4.bp.blogspot.com/-Ffs7LRM4v48/TbA9xl4XbzI/AAAAAAAAAgI/OG1U3O6Oluk/s320/best-worst.jpg" width="280" /></a></div>This is particularly good at creating bizarre, new, portmanteau-ish words. For example, if your source is the US states, you get <a href="http://www.reddit.com/r/programming/comments/9z4ds/i_made_a_markov_text_analyzergenerator_and_input/c0f3q1r">such gems as</a> - <i>Floridaho</i>, <i>Oklabama</i>, and <i>Flork</i>.<br /><br />The drawback to this approach is that it's no good at sentences, since what you'll get is likely to be a nonsensical collection of made-up words.<br /><br />Alternatively, you could work with pairs of words, or indeed <a href="http://en.wikipedia.org/wiki/N-gram">n-grams</a> of any size. This has the benefit of creating more readable-text, but at the cost of variation.<br /><br />Similarly, a smaller source text can produce lots of sentence fragments that never vary, while a large source can will give greater randomness.<br /><br />It all ultimately comes down to getting a desirable balance between variability and comprehensibility.<br /><br /><br /><b>Stuff to Play With</b><br /><br />I threw together this quick and dirty implementation in <a href="http://dl.dropbox.com/u/4635169/markov.py">python</a> - mostly just to show that I could. I ran it on the first few paragraphs of this post (pre-editing), and got this as an example output,<br /><blockquote><i>almost seems human but while no-one is going on some spam was text  that has anyone really should you might have postulated that has anyone  really</i></blockquote><a href="http://thatcan.be/my/next/tweet">That Can Be My Next Tweet</a>, mentioned above.<br /><br />On <a href="http://tweetcloud.com/">TweetCloud</a>, you can enter a word and get a word cloud of words that commonly follow that word in tweets. Word. <br /><br /><a href="http://ohthehugemanatee.net/word-o-matic/">Word-O-Matic</a>, also mention above, creates words based on any source text you give it. See also: associated <a href="http://www.reddit.com/r/programming/comments/9z4ds/i_made_a_markov_text_analyzergenerator_and_input/">Reddit thread</a> with lots of example results.<br /><br /><a href="http://www.beetleinabox.com/mkv_input.html">Markov Text Synthesizer</a>, a general online generator you can play with.<br /><br />There are various Twitter-bot attempts <a href="http://www.reddit.com/r/programming/comments/c6o1t/i_created_a_markov_text_generator_that_learns/">here</a>.<br /><br /><a href="http://www.devjason.com/2010/12/28/shakespeare-sonnet-sourced-markov-text-generation/">Markov Shakespearean Sonnet</a> (uses a slightly more complex generation method)<br /><br /><a href="http://www.codinghorror.com/blog/2008/06/markov-and-you.html">Someone</a> who explains it better than me.<br /><br />Oh, and you can also use Markov Chains to <a href="http://thepasqualian.com/?p=1831">generate music</a>.<br /><br /><br />Enjoy. <br /><br /><br />Oatzy.<br /><br /><br />[Spam is weirdly hard to come by these days.]